{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNa7i7md4k2FeZpg+d3hEs2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kartikee12/DAV/blob/main/Exp7_DAV_12.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Aim :** Perform the steps involved in Text Analytics in Python & R.\n",
        "\n",
        "**Theory :**\n",
        "\n",
        "**Python Text Analytics Libraries:**\n",
        "\n",
        "**1) NLTK (Natural Language Toolkit):**\n",
        "\n",
        "Features:\n",
        "\n",
        "a) Tokenization, stemming, lemmatization.\n",
        "\n",
        "b) Part-of-speech tagging, named entity recognition.\n",
        "\n",
        "c) Concordance and collocation analysis.\n",
        "\n",
        "Applications:\n",
        "\n",
        "a) Sentiment analysis, text classification.\n",
        "\n",
        "b)Information retrieval, language modeling.\n",
        "\n",
        "**2) spaCy:**\n",
        "\n",
        "Features:\n",
        "\n",
        "a) Tokenization, POS tagging, and named entity recognition.\n",
        "\n",
        "b) Dependency parsing, sentence segmentation.\n",
        "\n",
        "c) Pre-trained models for multiple languages.\n",
        "\n",
        "Applications:\n",
        "\n",
        "a) Named entity recognition, information extraction.\n",
        "\n",
        "b) Natural language understanding in chatbots.\n",
        "\n",
        "**3) TextBlob:**\n",
        "\n",
        "Features:\n",
        "\n",
        "a) Simple API for common NLP tasks.\n",
        "\n",
        "b) Sentiment analysis, noun phrase extraction.\n",
        "\n",
        "c) Part-of-speech tagging.\n",
        "\n",
        "Applications:\n",
        "\n",
        "a) Sentiment analysis of customer reviews.\n",
        "\n",
        "b) Basic text processing for beginners.\n",
        "\n",
        "**4) Gensim:**\n",
        "\n",
        "Features:\n",
        "\n",
        "a) Topic modeling (e.g., LDA).\n",
        "\n",
        "b) Document similarity analysis.\n",
        "\n",
        "c) Word embedding models (Word2Vec, Doc2Vec).\n",
        "\n",
        "Applications:\n",
        "\n",
        "a) Topic modeling in large document collections.\n",
        "\n",
        "b) Document similarity and clustering.\n",
        "\n",
        "**5) Transformers (Hugging Face):**\n",
        "\n",
        "Features:\n",
        "\n",
        "a) State-of-the-art pre-trained models (e.g., BERT, GPT).\n",
        "\n",
        "b) Easy integration for various NLP tasks.\n",
        "\n",
        "c) Fine-tuning capabilities.\n",
        "\n",
        "Applications:\n",
        "\n",
        "a) Named entity recognition, sentiment analysis.\n",
        "\n",
        "b) Text generation, language translation.\n",
        "\n",
        "**R Text Analytics Libraries:**\n",
        "\n",
        "**1) tm (Text Mining Package):**\n",
        "\n",
        "Features:\n",
        "\n",
        "a) Text preprocessing: Cleaning, stemming, stopword removal.\n",
        "\n",
        "b) Document-term matrix creation.\n",
        "\n",
        "c) Basic text mining functions.\n",
        "\n",
        "Applications:\n",
        "\n",
        "a) Topic modeling, clustering.\n",
        "\n",
        "b) Sentiment analysis, document classification.\n",
        "\n",
        "**2) quanteda:**\n",
        "\n",
        "Features:\n",
        "\n",
        "a) Fast and flexible text processing.\n",
        "\n",
        "b) Tokenization, n-grams, and corpus analysis.\n",
        "\n",
        "c) Support for advanced text analysis functions.\n",
        "\n",
        "Applications:\n",
        "\n",
        "a) Content analysis, sentiment analysis.\n",
        "\n",
        "b) Document-feature matrix creation.\n",
        "\n",
        "**3) tm.plugin.sentiment:**\n",
        "\n",
        "Features:\n",
        "\n",
        "a) Sentiment analysis using pre-trained models.\n",
        "\n",
        "b) Integration with the tm package.\n",
        "\n",
        "Applications:\n",
        "\n",
        "a) Analyzing sentiment in textual data.\n",
        "\n",
        "**4) textTinyR:**\n",
        "\n",
        "Features:\n",
        "\n",
        "a) Efficient text classification.\n",
        "\n",
        "b) Lightweight implementation for quick analysis.\n",
        "\n",
        "Applications:\n",
        "\n",
        "a) Fast text classification tasks.\n",
        "\n",
        "**5) quanteda.textstats:**\n",
        "\n",
        "Features:\n",
        "\n",
        "a) Basic text statistics.\n",
        "\n",
        "b) Word frequency analysis, readability scores.\n",
        "\n",
        "Applications:\n",
        "\n",
        "a) Exploratory text analysis.\n",
        "\n",
        "b) Extracting insights from text data.\n",
        "\n",
        "**Steps Involved in Text Analytics:**\n",
        "\n",
        "**1) Data Collection:** Gather text data from various sources such as documents, social media, or websites.\n",
        "\n",
        "**2) Text Preprocessing:** Clean and preprocess the text data by removing noise, handling missing values, and performing tasks like tokenization and stemming.\n",
        "\n",
        "**3) Text Exploration:** Explore the data through descriptive statistics, word frequency analysis, and visualization.\n",
        "\n",
        "**4) Feature Extraction:** Convert the text into a numerical format, often using techniques like TF-IDF (Term Frequency-Inverse Document Frequency).\n",
        "\n",
        "**5)Model Building:** Apply machine learning or natural language processing models for tasks such as sentiment analysis, classification, or clustering.\n",
        "\n",
        "**6) Evaluation:** Assess the performance of the models using appropriate metrics based on the task.\n",
        "\n",
        "**7) Visualization:** Visualize the results to gain insights and interpret the findings.\n",
        "\n",
        "**Benefits of Text Analytics:**\n",
        "\n",
        "**1) Insight Generation:** Extract valuable insights from large volumes of unstructured text data.\n",
        "\n",
        "**2) Decision Support:** Support decision-making processes by providing data-driven insights.\n",
        "\n",
        "**3) Automation:** Automate the analysis of textual data, saving time and resources.\n",
        "\n",
        "**4) Customer Feedback Analysis:** Understand customer sentiments and opinions from reviews and feedback.\n",
        "\n",
        "**5) Information Retrieval:** Enhance search functionalities by improving the relevance of retrieved information.\n",
        "\n",
        "**6) Fraud Detection:** Identify patterns or anomalies in textual data that may indicate fraudulent activities.\n",
        "\n",
        "**7) Competitive Intelligence:** Monitor and analyze competitor activities and market trends from textual sources.\n",
        "\n",
        "**Conclusion :** Text analytics is a powerful tool for extracting valuable information from text data, and the choice of libraries and techniques depends on the specific goals and characteristics of the data at hand.\n"
      ],
      "metadata": {
        "id": "CnNA_OXh0Epd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenization (Sentence & Word) in python**"
      ],
      "metadata": {
        "id": "jQVER6rRHnVV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lY8VHbjHJjRJ",
        "outputId": "91dc0dd1-6280-48ce-da16-130081b0b61b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "text = \"This is a sample text. Tokenize it!\"\n",
        "sent_tokens = sent_tokenize(text)\n",
        "word_tokens = word_tokenize(text)\n",
        "\n",
        "print(sent_tokens)\n",
        "print(word_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0H4hRf52Pt0X",
        "outputId": "c0d6f3d4-2309-4a1f-8d46-52f5efdc7177"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This is a sample text.', 'Tokenize it!']\n",
            "['This', 'is', 'a', 'sample', 'text', '.', 'Tokenize', 'it', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Frequency Distribution in python**"
      ],
      "metadata": {
        "id": "OeZcY4MgKFxP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import FreqDist\n",
        "fdist = FreqDist(word_tokens)\n",
        "print(fdist)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHj18i3ZLpAv",
        "outputId": "eb296979-e550-4547-903a-baad84ae2ce1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<FreqDist with 9 samples and 9 outcomes>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Frequency Distribution in R**"
      ],
      "metadata": {
        "id": "GyVaof7ELxJq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext rpy2.ipython"
      ],
      "metadata": {
        "id": "1tIju1uqL27Y"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "library(tm)\n",
        "\n",
        "text <- \"This is a sample text. Tokenize it!\"\n",
        "corpus <- Corpus(VectorSource(text))\n",
        "\n",
        "clean_text <- function(text) {\n",
        "  text <- tolower(text)\n",
        "  text <- removePunctuation(text)\n",
        "  text <- removeWords(text, stopwords(\"en\"))\n",
        "  return(text)\n",
        "}\n",
        "\n",
        "corpus_cleaned <- tm_map(corpus, content_transformer(clean_text))\n",
        "\n",
        "cleaned_text <- sapply(corpus_cleaned, function(x) as.character(x))\n",
        "print(cleaned_text)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KnLs5zQjL_nI",
        "outputId": "efa11eff-fe21-4be4-e179-74f5a103d382"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: Loading required package: NLP\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] \"   sample text tokenize \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Remove stopwords & punctuations in python**"
      ],
      "metadata": {
        "id": "IQPX9NxpMhbZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "text = \"This is a sample text. Tokenize it!\"\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "filtered_tokens = [word.lower() for word in tokens if (word.isalpha() and word.lower() not in stop_words)]\n",
        "\n",
        "print(filtered_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpleySj3MfvU",
        "outputId": "8d7b9d7c-9c15-4358-c567-a4757ed78bc7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['sample', 'text', 'tokenize']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Remove stopwords & punctuations in R**"
      ],
      "metadata": {
        "id": "lUAcyrqxM9yu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "library(tm)\n",
        "\n",
        "text <- \"This is a sample text. Tokenize it!\"\n",
        "corpus <- Corpus(VectorSource(text))\n",
        "\n",
        "corpus <- tm_map(corpus, content_transformer(tolower))\n",
        "corpus <- tm_map(corpus, removePunctuation)\n",
        "\n",
        "corpus <- tm_map(corpus, removeWords, stopwords(\"english\"))\n",
        "\n",
        "cleaned_text <- sapply(corpus, function(x) as.character(x))\n",
        "\n",
        "print(cleaned_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKRfhluSNFF0",
        "outputId": "61b9741b-9531-4a52-e7d2-55966635d488"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] \"   sample text tokenize \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lexicon Normalization (Stemming, Lemmatization) in Python**"
      ],
      "metadata": {
        "id": "2De10EZLEsPE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRu9G751QVnB",
        "outputId": "c99909c3-bdd4-48d0-adce-c81356722733"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "porter = PorterStemmer()\n",
        "words = [\"running\", \"flies\", \"happily\", \"better\"]\n",
        "stemmed_words = [porter.stem(word) for word in words]\n",
        "print(stemmed_words)\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "words = [\"running\", \"flies\", \"happily\", \"better\"]\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "print(lemmatized_words)\n"
      ],
      "metadata": {
        "id": "AJQUkV44NT6A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b8bc0e6-8f99-4b41-ff06-8282bf2048d7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['run', 'fli', 'happili', 'better']\n",
            "['running', 'fly', 'happily', 'better']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lexicon Normalization (Stemming, Lemmatization) in R**"
      ],
      "metadata": {
        "id": "if4don8iGD28"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext rpy2.ipython"
      ],
      "metadata": {
        "id": "Ed9015PjGmot",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3668c249-e476-48ab-d8ac-734b0e7edd70"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The rpy2.ipython extension is already loaded. To reload it, use:\n",
            "  %reload_ext rpy2.ipython\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "library(tm)\n",
        "library(textTinyR)\n",
        "library(SnowballC)\n",
        "words <- c(\"running\", \"flies\", \"happily\", \"better\")\n",
        "corpus <- Corpus(VectorSource(words))\n",
        "corpus <- tm_map(corpus, content_transformer(stemDocument))\n",
        "stemmed_words <- sapply(corpus, function(x) unlist(strsplit(as.character(x), \" \")))\n",
        "print(stemmed_words)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ov_Hn1P-GIam",
        "outputId": "c2032356-8a39-4770-a5f7-a58f2cd39da7"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] \"run\"     \"fli\"     \"happili\" \"better\" \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part of Speech Tagging in Python**"
      ],
      "metadata": {
        "id": "tXLc6vKk3QJS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLRZciPaQul0",
        "outputId": "aaaa761c-8cc1-425a-fef6-63b98ac4b3d7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import pos_tag\n",
        "pos_tags = pos_tag(words)\n"
      ],
      "metadata": {
        "id": "hNtVGf9e67nH"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Named Entity Recognition in Python**"
      ],
      "metadata": {
        "id": "HsdrfEvsVSFt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(text)\n",
        "entities = [(ent.text, ent.label_) for ent in doc.ents]\n"
      ],
      "metadata": {
        "id": "-gvh_gvX7A27"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Web Scraping in Python**"
      ],
      "metadata": {
        "id": "cPOkqBCwVXcJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "url = \"https://example.com\"\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.text, \"html.parser\")\n"
      ],
      "metadata": {
        "id": "3KfGNu5g7Ehf"
      },
      "execution_count": 18,
      "outputs": []
    }
  ]
}